# ğŸ“Š Principal Component Regression (PCR) for Dimensionality Reduction & Predictive Modeling  

This is my **[22]th project** in the field of **Machine Learning and Regression Modeling**.  

This project demonstrates how to implement **Principal Component Regression (PCR)** â€” a technique that combines **Principal Component Analysis (PCA)** with regression modeling to handle **multicollinearity** and reduce dimensionality â€” to predict target variables based on multiple real-world features.  

It showcases a complete regression pipeline â€” from **data exploration to model training and evaluation** â€” offering a strong foundation for solving prediction tasks in industries such as **finance, healthcare, marketing, and engineering**.  

---

## ğŸ“˜ Project Overview  

**Principal Component Regression (PCR)** leverages PCA to extract uncorrelated features (principal components) and then fits a regression model on these components.  
This approach:  
- âœ… Reduces noise  
- âœ… Improves interpretability  
- âœ… Mitigates instability when predictors are highly correlated  

### ğŸ”‘ End-to-End Workflow in this Notebook:  
- ğŸ“¥ **Loading and inspecting the dataset**  
- ğŸ” **Exploratory Data Analysis (EDA)** using *Seaborn* and *Matplotlib*  
- ğŸ§¼ **Preprocessing and scaling features**  
- âœ‚ **Splitting the dataset** using `train_test_split()`  
- ğŸ§  **Training** a regression model with `Pipeline([Scaler â†’ PCA â†’ LinearRegression])`  
- ğŸ“ˆ **Evaluating the model** using:  
  - Mean Absolute Error (MAE)  
  - Mean Squared Error (MSE)  
  - Root Mean Squared Error (RMSE)  
  - RÂ² Score  
- ğŸ“Š **Visualizations**: explained variance ratios & predicted vs. actual values  

---

## ğŸ“Š About the Dataset  

The dataset simulates real-world scenarios where predictors are **interrelated**, leading to **multicollinearity**. It includes:  
- ğŸ“¢ **Feature Variables** â€” Numerical indicators with correlations among them  
- ğŸ“‰ **Target Variable** â€” Continuous numerical variable representing the prediction goal  

---

## âœ… Features Implemented  

- Data loading and inspection  
- Missing value checks and statistical summaries  
- EDA with correlation heatmaps and variance plots  
- Dimensionality reduction using PCA  
- Model training with Linear Regression on principal components  
- Model performance evaluation (MAE, MSE, RMSE, RÂ²)  
- Visualization of explained variance & prediction accuracy  

---

## ğŸ§ª Technologies Used  

- Python 3.x  
- Pandas  
- NumPy  
- Seaborn  
- Matplotlib  
- Scikit-learn  

---

## ğŸ“‚ Use Cases  

This notebook is ideal for:  
- ğŸ“š Learning how to implement **Principal Component Regression**  
- ğŸ“ˆ Predictive modeling where features are highly correlated  
- ğŸ§  Reducing dimensionality for large feature spaces  
- ğŸ§³ Building a strong **data science portfolio project**  

---

## ğŸ‘¨â€ğŸ’» Author  

**Zohaib Sattar**  
ğŸ“§ Email: [zabizubi86@gmail.com](mailto:zabizubi86@gmail.com)  
ğŸ”— LinkedIn: [Zohaib Sattar](https://www.linkedin.com/in/zohaib-sattar)  

---

## â­ Support the Project  

If this project helped you understand **dimensionality reduction** and **regression modeling**, please consider giving it a â­ on GitHub and sharing it with your peers.  

Your support encourages more open-source contributions and helps grow the ML community! ğŸš€  
